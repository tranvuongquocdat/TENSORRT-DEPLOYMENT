{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'ONNXClassifierWrapper' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn [7], line 4\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39monnx\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mhelper\u001b[39;00m\n\u001b[0;32m      3\u001b[0m N_CLASSES \u001b[39m=\u001b[39m \u001b[39m1000\u001b[39m \u001b[39m# Our ResNet-50 is trained on a 1000 class ImageNet task\u001b[39;00m\n\u001b[1;32m----> 4\u001b[0m trt_model \u001b[39m=\u001b[39m ONNXClassifierWrapper(\u001b[39m\"\u001b[39m\u001b[39mresnet_engine.trt\u001b[39m\u001b[39m\"\u001b[39m, [BATCH_SIZE, N_CLASSES], target_dtype \u001b[39m=\u001b[39m PRECISION)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'ONNXClassifierWrapper' is not defined"
     ]
    }
   ],
   "source": [
    "#deploy ONNX model\n",
    "import onnx.helper\n",
    "N_CLASSES = 1000 # Our ResNet-50 is trained on a 1000 class ImageNet task\n",
    "trt_model = ONNXClassifierWrapper(\"resnet_engine.trt\", [BATCH_SIZE, N_CLASSES], target_dtype = PRECISION)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/resnet/resnet50_weights_tf_dim_ordering_tf_kernels.h5\n",
      "102967424/102967424 [==============================] - 13s 0us/step\n"
     ]
    }
   ],
   "source": [
    "#ONNX conversion and deployment\n",
    "from tensorflow.keras.applications import ResNet50\n",
    "\n",
    "model = ResNet50(weights = 'imagenet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#convert Resnet50 from h5 to onnx_model\n",
    "import tf2onnx\n",
    "\n",
    "model.save('my_model')\n",
    "!python -m tf2onnx.convert --saved-model my_model --output temp.onnx\n",
    "onnx_model = onnx.load_model('temp.onnx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Set an explicit batch size in the ONNX file\n",
    "import onnx\n",
    "BATCH_SIZE = 64\n",
    "inputs = onnx_model.graph.input\n",
    "for input in inputs:\n",
    "    dim1 = input.type.tensor_type.shape.dim[0]\n",
    "    dim1.dim_value = BATCH_SIZE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Save the ONNX file\n",
    "model_name = \"resnet50_onnx_model.onnx\"\n",
    "onnx.save_model(onnx_model, model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\PC\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torchvision\\models\\_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and will be removed in 0.15, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "c:\\Users\\PC\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torchvision\\models\\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and will be removed in 0.15. The current behavior is equivalent to passing `weights=ResNeXt50_32X4D_Weights.IMAGENET1K_V1`. You can also use `weights=ResNeXt50_32X4D_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n",
      "Downloading: \"https://download.pytorch.org/models/resnext50_32x4d-7cdf4587.pth\" to C:\\Users\\PC/.cache\\torch\\hub\\checkpoints\\resnext50_32x4d-7cdf4587.pth\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e3dc88e869cb47c1bdccc5f108f63f8a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0.00/95.8M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#convert pytorch to ONNX\n",
    "\n",
    "#import resnet model\n",
    "import torchvision.models as models\n",
    "\n",
    "resnext50_32x4d = models.resnext50_32x4d(pretrained=True)\n",
    "\n",
    "#save ONNX file from pytorch\n",
    "import torch\n",
    "\n",
    "BATCH_SIZE = 64\n",
    "dummy_input=torch.randn(BATCH_SIZE, 3, 224, 224)\n",
    "\n",
    "#Save ONNX file\n",
    "import torch.onnx\n",
    "torch.onnx.export(resnext50_32x4d, dummy_input, \"resnet50_onnx_model(from Pytorch).onnx\", verbose=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "&&&& RUNNING TensorRT.trtexec [TensorRT v8402] # trtexec --onnx=resnet50_onnx_model.onnx --saveEngine=resnet_engine(pytorch).trt\n",
      "[10/31/2022-21:03:46] [I] === Model Options ===\n",
      "[10/31/2022-21:03:46] [I] Format: ONNX\n",
      "[10/31/2022-21:03:46] [I] Model: resnet50_onnx_model.onnx\n",
      "[10/31/2022-21:03:46] [I] Output:\n",
      "[10/31/2022-21:03:46] [I] === Build Options ===\n",
      "[10/31/2022-21:03:46] [I] Max batch: explicit batch\n",
      "[10/31/2022-21:03:46] [I] Memory Pools: workspace: default, dlaSRAM: default, dlaLocalDRAM: default, dlaGlobalDRAM: default\n",
      "[10/31/2022-21:03:46] [I] minTiming: 1\n",
      "[10/31/2022-21:03:46] [I] avgTiming: 8\n",
      "[10/31/2022-21:03:46] [I] Precision: FP32\n",
      "[10/31/2022-21:03:46] [I] LayerPrecisions: \n",
      "[10/31/2022-21:03:46] [I] Calibration: \n",
      "[10/31/2022-21:03:46] [I] Refit: Disabled\n",
      "[10/31/2022-21:03:46] [I] Sparsity: Disabled\n",
      "[10/31/2022-21:03:46] [I] Safe mode: Disabled\n",
      "[10/31/2022-21:03:46] [I] DirectIO mode: Disabled\n",
      "[10/31/2022-21:03:46] [I] Restricted mode: Disabled\n",
      "[10/31/2022-21:03:46] [I] Build only: Disabled\n",
      "[10/31/2022-21:03:46] [I] Save engine: resnet_engine(pytorch).trt\n",
      "[10/31/2022-21:03:46] [I] Load engine: \n",
      "[10/31/2022-21:03:46] [I] Profiling verbosity: 0\n",
      "[10/31/2022-21:03:46] [I] Tactic sources: Using default tactic sources\n",
      "[10/31/2022-21:03:46] [I] timingCacheMode: local\n",
      "[10/31/2022-21:03:46] [I] timingCacheFile: \n",
      "[10/31/2022-21:03:46] [I] Input(s)s format: fp32:CHW\n",
      "[10/31/2022-21:03:46] [I] Output(s)s format: fp32:CHW\n",
      "[10/31/2022-21:03:46] [I] Input build shapes: model\n",
      "[10/31/2022-21:03:46] [I] Input calibration shapes: model\n",
      "[10/31/2022-21:03:46] [I] === System Options ===\n",
      "[10/31/2022-21:03:46] [I] Device: 0\n",
      "[10/31/2022-21:03:46] [I] DLACore: \n",
      "[10/31/2022-21:03:46] [I] Plugins:\n",
      "[10/31/2022-21:03:46] [I] === Inference Options ===\n",
      "[10/31/2022-21:03:46] [I] Batch: Explicit\n",
      "[10/31/2022-21:03:46] [I] Input inference shapes: model\n",
      "[10/31/2022-21:03:46] [I] Iterations: 10\n",
      "[10/31/2022-21:03:46] [I] Duration: 3s (+ 200ms warm up)\n",
      "[10/31/2022-21:03:46] [I] Sleep time: 0ms\n",
      "[10/31/2022-21:03:46] [I] Idle time: 0ms\n",
      "[10/31/2022-21:03:46] [I] Streams: 1\n",
      "[10/31/2022-21:03:46] [I] ExposeDMA: Disabled\n",
      "[10/31/2022-21:03:46] [I] Data transfers: Enabled\n",
      "[10/31/2022-21:03:46] [I] Spin-wait: Disabled\n",
      "[10/31/2022-21:03:46] [I] Multithreading: Disabled\n",
      "[10/31/2022-21:03:46] [I] CUDA Graph: Disabled\n",
      "[10/31/2022-21:03:46] [I] Separate profiling: Disabled\n",
      "[10/31/2022-21:03:46] [I] Time Deserialize: Disabled\n",
      "[10/31/2022-21:03:46] [I] Time Refit: Disabled\n",
      "[10/31/2022-21:03:46] [I] Inputs:\n",
      "[10/31/2022-21:03:46] [I] === Reporting Options ===\n",
      "[10/31/2022-21:03:46] [I] Verbose: Disabled\n",
      "[10/31/2022-21:03:46] [I] Averages: 10 inferences\n",
      "[10/31/2022-21:03:46] [I] Percentile: 99\n",
      "[10/31/2022-21:03:46] [I] Dump refittable layers:Disabled\n",
      "[10/31/2022-21:03:46] [I] Dump output: Disabled\n",
      "[10/31/2022-21:03:46] [I] Profile: Disabled\n",
      "[10/31/2022-21:03:46] [I] Export timing to JSON file: \n",
      "[10/31/2022-21:03:46] [I] Export output to JSON file: \n",
      "[10/31/2022-21:03:46] [I] Export profile to JSON file: \n",
      "[10/31/2022-21:03:46] [I] \n",
      "[10/31/2022-21:03:46] [I] === Device Information ===\n",
      "[10/31/2022-21:03:46] [I] Selected Device: NVIDIA GeForce RTX 2060 SUPER\n",
      "[10/31/2022-21:03:46] [I] Compute Capability: 7.5\n",
      "[10/31/2022-21:03:46] [I] SMs: 34\n",
      "[10/31/2022-21:03:46] [I] Compute Clock Rate: 1.65 GHz\n",
      "[10/31/2022-21:03:46] [I] Device Global Memory: 8191 MiB\n",
      "[10/31/2022-21:03:46] [I] Shared Memory per SM: 64 KiB\n",
      "[10/31/2022-21:03:46] [I] Memory Bus Width: 256 bits (ECC disabled)\n",
      "[10/31/2022-21:03:46] [I] Memory Clock Rate: 7.001 GHz\n",
      "[10/31/2022-21:03:46] [I] \n",
      "[10/31/2022-21:03:46] [I] TensorRT version: 8.4.2\n",
      "[10/31/2022-21:03:47] [I] [TRT] [MemUsageChange] Init CUDA: CPU +535, GPU +0, now: CPU 8912, GPU 1523 (MiB)\n",
      "[10/31/2022-21:03:48] [I] [TRT] [MemUsageChange] Init builder kernel library: CPU +277, GPU +112, now: CPU 9385, GPU 1627 (MiB)\n",
      "[10/31/2022-21:03:48] [I] Start parsing network model\n",
      "[10/31/2022-21:03:48] [I] [TRT] ----------------------------------------------------------------\n",
      "[10/31/2022-21:03:48] [I] [TRT] Input filename:   resnet50_onnx_model.onnx\n",
      "[10/31/2022-21:03:48] [I] [TRT] ONNX IR version:  0.0.7\n",
      "[10/31/2022-21:03:48] [I] [TRT] Opset version:    13\n",
      "[10/31/2022-21:03:48] [I] [TRT] Producer name:    tf2onnx\n",
      "[10/31/2022-21:03:48] [I] [TRT] Producer version: 1.12.1 b6d590\n",
      "[10/31/2022-21:03:48] [I] [TRT] Domain:           \n",
      "[10/31/2022-21:03:48] [I] [TRT] Model version:    0\n",
      "[10/31/2022-21:03:48] [I] [TRT] Doc string:       \n",
      "[10/31/2022-21:03:48] [I] [TRT] ----------------------------------------------------------------\n",
      "[10/31/2022-21:03:48] [I] Finish parsing network model\n",
      "[10/31/2022-21:03:49] [I] [TRT] [MemUsageChange] Init cuBLAS/cuBLASLt: CPU +732, GPU +426, now: CPU 10102, GPU 2080 (MiB)\n",
      "[10/31/2022-21:03:50] [I] [TRT] [MemUsageChange] Init cuDNN: CPU +148, GPU +92, now: CPU 10250, GPU 2172 (MiB)\n",
      "[10/31/2022-21:03:50] [I] [TRT] Local timing cache in use. Profiling results in this builder pass will not be stored.\n",
      "[10/31/2022-21:04:20] [I] [TRT] Detected 1 inputs and 1 output network tensors.\n",
      "[10/31/2022-21:04:20] [I] [TRT] Total Host Persistent Memory: 82640\n",
      "[10/31/2022-21:04:20] [I] [TRT] Total Device Persistent Memory: 279552\n",
      "[10/31/2022-21:04:20] [I] [TRT] Total Scratch Memory: 191102976\n",
      "[10/31/2022-21:04:20] [I] [TRT] [MemUsageStats] Peak memory usage of TRT CPU/GPU memory allocators: CPU 16 MiB, GPU 4805 MiB\n",
      "[10/31/2022-21:04:20] [I] [TRT] [BlockAssignment] Algorithm ShiftNTopDown took 3.228ms to assign 4 blocks to 75 nodes requiring 546897920 bytes.\n",
      "[10/31/2022-21:04:20] [I] [TRT] Total Activation Memory: 546897920\n",
      "[10/31/2022-21:04:20] [I] [TRT] [MemUsageChange] Init cuBLAS/cuBLASLt: CPU +0, GPU +8, now: CPU 15454, GPU 1780 (MiB)\n",
      "[10/31/2022-21:04:20] [I] [TRT] [MemUsageChange] Init cuDNN: CPU +0, GPU +10, now: CPU 15454, GPU 1790 (MiB)\n",
      "[10/31/2022-21:04:20] [I] [TRT] [MemUsageChange] TensorRT-managed allocation in building engine: CPU +7, GPU +98, now: CPU 7, GPU 98 (MiB)\n",
      "[10/31/2022-21:04:20] [I] Engine built in 34.3461 sec.\n",
      "[10/31/2022-21:04:20] [I] [TRT] [MemUsageChange] Init CUDA: CPU +0, GPU +0, now: CPU 15282, GPU 1602 (MiB)\n",
      "[10/31/2022-21:04:20] [I] [TRT] Loaded engine size: 97 MiB\n",
      "[10/31/2022-21:04:20] [I] [TRT] [MemUsageChange] Init cuBLAS/cuBLASLt: CPU +0, GPU +10, now: CPU 15282, GPU 1710 (MiB)\n",
      "[10/31/2022-21:04:20] [I] [TRT] [MemUsageChange] Init cuDNN: CPU +0, GPU +8, now: CPU 15282, GPU 1718 (MiB)\n",
      "[10/31/2022-21:04:20] [I] [TRT] [MemUsageChange] TensorRT-managed allocation in engine deserialization: CPU +0, GPU +98, now: CPU 0, GPU 98 (MiB)\n",
      "[10/31/2022-21:04:20] [I] Engine deserialized in 0.0211605 sec.\n",
      "[10/31/2022-21:04:20] [I] [TRT] [MemUsageChange] Init cuBLAS/cuBLASLt: CPU +0, GPU +8, now: CPU 15281, GPU 1710 (MiB)\n",
      "[10/31/2022-21:04:20] [I] [TRT] [MemUsageChange] Init cuDNN: CPU +0, GPU +8, now: CPU 15281, GPU 1718 (MiB)\n",
      "[10/31/2022-21:04:21] [I] [TRT] [MemUsageChange] TensorRT-managed allocation in IExecutionContext creation: CPU +0, GPU +521, now: CPU 0, GPU 619 (MiB)\n",
      "[10/31/2022-21:04:21] [I] Using random values for input input_1\n",
      "[10/31/2022-21:04:21] [I] Created input binding for input_1 with dimensions 64x224x224x3\n",
      "[10/31/2022-21:04:21] [I] Using random values for output predictions\n",
      "[10/31/2022-21:04:21] [I] Created output binding for predictions with dimensions 64x1000\n",
      "[10/31/2022-21:04:21] [I] Starting inference\n",
      "[10/31/2022-21:04:24] [I] Warmup completed 3 queries over 200 ms\n",
      "[10/31/2022-21:04:24] [I] Timing trace has 37 queries over 3.23823 s\n",
      "[10/31/2022-21:04:24] [I] \n",
      "[10/31/2022-21:04:24] [I] === Trace details ===\n",
      "[10/31/2022-21:04:24] [I] Trace averages of 10 runs:\n",
      "[10/31/2022-21:04:24] [I] Average on 10 runs - GPU latency: 85.037 ms - Host latency: 88.2469 ms (enqueue 1.10096 ms)\n",
      "[10/31/2022-21:04:24] [I] Average on 10 runs - GPU latency: 85.7769 ms - Host latency: 88.9944 ms (enqueue 1.16861 ms)\n",
      "[10/31/2022-21:04:24] [I] Average on 10 runs - GPU latency: 85.5833 ms - Host latency: 88.7906 ms (enqueue 0.905835 ms)\n",
      "[10/31/2022-21:04:24] [I] \n",
      "[10/31/2022-21:04:24] [I] === Performance summary ===\n",
      "[10/31/2022-21:04:24] [I] Throughput: 11.426 qps\n",
      "[10/31/2022-21:04:24] [I] Latency: min = 83.3508 ms, max = 94.8851 ms, mean = 88.3694 ms, median = 88.135 ms, percentile(99%) = 94.8851 ms\n",
      "[10/31/2022-21:04:24] [I] Enqueue Time: min = 0.616577 ms, max = 4.69409 ms, mean = 1.20227 ms, median = 0.931641 ms, percentile(99%) = 4.69409 ms\n",
      "[10/31/2022-21:04:24] [I] H2D Latency: min = 3.11987 ms, max = 3.27734 ms, mean = 3.1793 ms, median = 3.18066 ms, percentile(99%) = 3.27734 ms\n",
      "[10/31/2022-21:04:24] [I] GPU Compute Time: min = 80.1362 ms, max = 91.646 ms, mean = 85.152 ms, median = 84.9213 ms, percentile(99%) = 91.646 ms\n",
      "[10/31/2022-21:04:24] [I] D2H Latency: min = 0.0327148 ms, max = 0.0905762 ms, mean = 0.0381338 ms, median = 0.0361633 ms, percentile(99%) = 0.0905762 ms\n",
      "[10/31/2022-21:04:24] [I] Total Host Walltime: 3.23823 s\n",
      "[10/31/2022-21:04:24] [I] Total GPU Compute Time: 3.15062 s\n",
      "[10/31/2022-21:04:24] [I] Explanations of the performance metrics are printed in the verbose logs.\n",
      "[10/31/2022-21:04:24] [I] \n",
      "&&&& PASSED TensorRT.trtexec [TensorRT v8402] # trtexec --onnx=resnet50_onnx_model.onnx --saveEngine=resnet_engine(pytorch).trt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[10/31/2022-21:03:48] [W] [TRT] onnx2trt_utils.cpp:369: Your ONNX model has been generated with INT64 weights, while TensorRT does not natively support INT64. Attempting to cast down to INT32.\n",
      "[10/31/2022-21:04:20] [W] [TRT] The getMaxBatchSize() function should not be used with an engine built from a network created with NetworkDefinitionCreationFlag::kEXPLICIT_BATCH flag. This function will always return 1.\n",
      "[10/31/2022-21:04:20] [W] [TRT] The getMaxBatchSize() function should not be used with an engine built from a network created with NetworkDefinitionCreationFlag::kEXPLICIT_BATCH flag. This function will always return 1.\n",
      "[10/31/2022-21:04:24] [W] * GPU compute time is unstable, with coefficient of variance = 3.64305%.\n",
      "[10/31/2022-21:04:24] [W]   If not already in use, locking GPU clock frequency or adding --useSpinWait may improve the stability.\n"
     ]
    }
   ],
   "source": [
    "#convert ONNX to TensorRT\n",
    "!trtexec --onnx=resnet50_onnx_model.onnx --saveEngine=resnet_engine(pytorch).trt"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.7 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "e593ac106456af50ce7af38f9671c411b49d6cd90f9b885e167f0f594e09038c"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
